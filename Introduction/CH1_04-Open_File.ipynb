{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<font size = \"5\"> **Chapter 1: [Introduction](CH1_00-Introduction.ipynb)** </font>\n",
    "\n",
    "\n",
    "<hr style=\"height:1px;border-top:4px solid #FF8200\" />\n",
    "\n",
    "# Open DM3 Images, Spectra, Spectrum-Images and  Image-Stacks with pyNSID \n",
    "\n",
    "[Download](https://raw.githubusercontent.com/gduscher/MSE672-Introduction-to-TEM//main/Introduction/CH1_04-Open_File.ipynb)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "part of \n",
    "\n",
    "<font size = \"5\"> **[MSE672:  Introduction to Transmission Electron Microscopy](../_MSE672_Intro_TEM.ipynb)**</font>\n",
    "\n",
    "by Gerd Duscher, Spring 2022\n",
    "\n",
    "Microscopy Facilities<br>\n",
    "Institute of Advanced Materials & Manufacturing<br>\n",
    "Materials Science & Engineering<br>\n",
    "The University of Tennessee, Knoxville\n",
    "\n",
    "Background and methods to analysis and quantification of data acquired with transmission electron microscopes.\n",
    "\n",
    "---\n",
    "Reading a dm file and translating the data in a **[pyNSID](https://pycroscopy.github.io/pyNSID/)** style hf5py file to be compatible with  the **[pycroscopy](https://pycroscopy.github.io/pycroscopy/)** package.\n",
    "\n",
    "Because, many other packages and programs for TEM data manipulation are based on the ``hdf5`` file-formats it is relatively easy to convert back and forward between them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Import packages for figures and\n",
    "### Check Installed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from pkg_resources import get_distribution, DistributionNotFound\n",
    "\n",
    "def test_package(package_name):\n",
    "    \"\"\"Test if package exists and returns version or -1\"\"\"\n",
    "    try:\n",
    "        version = get_distribution(package_name).version\n",
    "    except (DistributionNotFound, ImportError):\n",
    "        version = '-1'\n",
    "    return version\n",
    "\n",
    "if test_package('pyTEMlib') < '0.2022.1.0':\n",
    "        print('installing pyTEMlib')\n",
    "        !{sys.executable} -m pip install  --upgrade pyTEMlib -q\n",
    "# ------------------------------\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the plotting and figure packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pylab --no-import-all notebook\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../pyTEMlib')\n",
    "import pyTEMlib\n",
    "import pyTEMlib.file_tools  as ft     # File input/ output library\n",
    "\n",
    "import sidpy\n",
    "import pyNSID\n",
    "import h5py\n",
    "\n",
    "# For archiving reasons it is a good idea to print the version numbers out at this point\n",
    "print('pyTEM version: ',pyTEMlib.__version__)\n",
    "__notebook__='CH1_04-Reading_File'\n",
    "__notebook_version__='2021_12_14'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Open a file \n",
    "\n",
    "This function opens a hfd5 file in the pyNSID style which enables you to keep track of your data analysis.\n",
    "\n",
    "Please see the **[Installation](CH1_02-Prerequisites.ipynb#TEM-Library)** notebook for installation.\n",
    "\n",
    "We want to consolidate files into one dataset that belongs together.  For example a spectrum image dataset consists of: \n",
    "* Survey image, \n",
    "* EELS spectra \n",
    "* Z-contrast image acquired simultaneously with the spectra.\n",
    "\n",
    "\n",
    "So load the top dataset first in the above example the survey image.\n",
    "\n",
    "Please note that the plotting routine of ``matplotlib`` was introduced in **[Matplotlib and Numpy for Micrographs](CH1_03-Data_Representation.ipynb)** notebook.\n",
    "\n",
    "**Use the file p1-3hr.dm3 from TEM_data directory for a practice run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ------ Input ------- #\n",
    "load_example = True\n",
    "# -------------------- #\n",
    "\n",
    "# Open file widget and select file which will be opened in code cell below\n",
    "if not load_example:\n",
    "    drive_directory = ft.get_last_path()\n",
    "    file_widget = ft.FileWidget(drive_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    main_dataset.h5_dataset.file.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if load_example:\n",
    "    file_name = '../example_data/p1-3-hr3.dm3'\n",
    "else:\n",
    "    file_name = file_widget.file_name\n",
    "\n",
    "main_dataset = ft.open_file(file_name)\n",
    "current_channel = main_dataset.h5_dataset.parent.parent\n",
    "\n",
    "view = main_dataset.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "The data themselves reside in a ``sidpy dataset`` which we name ``current_dataset``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current_dataset has additional information stored as attributes which can be accessed through their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(main_dataset)\n",
    "main_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'size of current dataset is {main_dataset.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current_dataset has additional information stored as attributes which can be accessed through their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('title: ', main_dataset.title)\n",
    "print('data type: ', main_dataset.data_type)\n",
    "main_dataset.metadata\n",
    "for key in current_channel:\n",
    "    try:\n",
    "        if key in current_channel[key]:\n",
    "            print(current_channel[key][key]['original_metadata'].attrs.keys())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Structure\n",
    "The current_channel (like a directory in a file system) contains several groups.\n",
    "\n",
    "Below I show how to access one of those groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dataset = main_dataset\n",
    "print(current_channel.keys())\n",
    "def add_data(dataset, h5_group=None):\n",
    "    \"\"\"Write data to hdf5 file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: sidpy.Dataset\n",
    "        data to write to file\n",
    "    h5_group: None, sidpy.Dataset, h5py.Group, h5py.Dataset, h5py.File\n",
    "        identifier to which group the data are added (if None the dataset must have a valid h5_dataset)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    log_group: h5py.Dataset\n",
    "        reference the dataset has been written to. (is also stored in h5_dataset attribute of sidpy.Dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    if h5_group is None:\n",
    "        if isinstance(dataset.h5_dataset, h5py.Dataset):\n",
    "            h5_group = dataset.h5_dataset.parent.parent.parent\n",
    "    if isinstance(h5_group, h5py.Dataset):\n",
    "        h5_group = h5_group.parent.parent.parent\n",
    "    elif isinstance(h5_group, sidpy.Dataset):\n",
    "        h5_group = h5_group.h5_dataset.parent.parent.parent\n",
    "    elif isinstance(h5_group, h5py.File):\n",
    "        h5_group = h5_group['Measurement_000']\n",
    "        \n",
    "    if not isinstance(h5_group, h5py.Group):\n",
    "        raise TypeError('Need a valid identifier for a hdf5 group to store data in')\n",
    "\n",
    "    log_group = sidpy.hdf.prov_utils.create_indexed_group(h5_group, 'Channel_')\n",
    "    h5_dataset = pyNSID.hdf_io.write_nsid_dataset(dataset, log_group)\n",
    "    \n",
    "    if hasattr(dataset, 'meta_data'):\n",
    "        if 'analysis' in dataset.meta_data:\n",
    "            log_group['analysis'] = dataset.meta_data['analysis']\n",
    "            \n",
    "    dataset.h5_dataset = h5_dataset\n",
    "    return h5_dataset\n",
    "\n",
    "print(current_channel)\n",
    "current_dataset.metadata= {'a': 'nix', 'b': 'nada'}\n",
    "#new_data = pyNSID.hdf_io.write_results(current_channel.parent, dataset=current_dataset)\n",
    "new_data = add_data(current_dataset, h5_group=None)\n",
    "\n",
    "print(current_dataset.h5_dataset)\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important attribute in ``current_dataset`` is the ``original_metadata`` group, where all the original metadata of your file reside in the ``attributes``. This is usually a long list for ``dm3`` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dataset.h5_dataset.parent['original_metadata'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in current_dataset.h5_dataset.parent['original_metadata'].attrs.items():\n",
    "    print(key, value)\n",
    "print(current_dataset.h5_dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(current_channel.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data\n",
    "\n",
    "To add another dataset that belongs to this measurement we will use the **h5_add_channel** from  **file_tools** in the  pyTEMlib package.\n",
    "\n",
    "Here is how we add a channel there.\n",
    "\n",
    "We can also add a new measurement group (add_measurement in pyTEMlib) for similar datasets.\n",
    "\n",
    "This is equivalent to making a new directory in a file structure on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyNSID\n",
    "\n",
    "def add_dataset(dataset, h5_group=None):\n",
    "    \"\"\"Write data to hdf5 file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: sidpy.Dataset\n",
    "        data to write to file\n",
    "    h5_group: None, sidpy.Dataset, h5py.Group, h5py.Dataset, h5py.File\n",
    "        identifier to which group the data are added (if None the dataset must have a valid h5_dataset)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    h5_dataset: h5py.Dataset\n",
    "        reference the dataset has been written to. (is also stored in h5_dataset attribute of sidpy.Dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    if h5_group is None:\n",
    "        if isinstance(dataset.h5_dataset, h5py.Dataset):\n",
    "            h5_group = dataset.h5_dataset.parent.parent.parent\n",
    "    if isinstance(h5_group, h5py.Dataset):\n",
    "        h5_group = h5_group.parent.parent.parent\n",
    "    elif isinstance(h5_group, sidpy.Dataset):\n",
    "        h5_group = h5_group.h5_dataset.parent.parent.parent\n",
    "    elif isinstance(h5_group, h5py.File):\n",
    "        h5_group = h5_group['Measurement_000']\n",
    "\n",
    "    if not isinstance(h5_group, h5py.Group):\n",
    "        raise TypeError('Need a valid identifier for a hdf5 group to store data in')\n",
    "\n",
    "    log_group = sidpy.hdf.prov_utils.create_indexed_group(h5_group, 'Channel_')\n",
    "    h5_dataset = pyNSID.hdf_io.write_nsid_dataset(dataset, log_group)\n",
    "\n",
    "    if hasattr(dataset, 'meta_data'):\n",
    "        if 'analysis' in dataset.meta_data:\n",
    "            log_group['analysis'] = dataset.meta_data['analysis']\n",
    "\n",
    "    dataset.h5_dataset = h5_dataset\n",
    "    return h5_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use above functions to add the content of a (random) data-file to the current file.\n",
    "\n",
    "This is important if you for example want to add a Z-contrast or survey-image to a spectrum image.\n",
    "\n",
    "Therefore, these functions enable you to collect the data from different files that belong together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_channel = h5_add_channel(current_channel)\n",
    "add_dataset(current_dataset, current_channel.parent)\n",
    "\n",
    "ft.h5_tree(current_channel)  #wraps sidpy.hdf_utils.print_tree(h5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding additional information\n",
    "\n",
    "Similarly, we can add a whole new measurement group or a structure group.\n",
    "\n",
    "This function will be contained in the KinsCat package of pyTEMlib.\n",
    "\n",
    "If you loaded the example image, with graphite and ZnO both are viewed in the [1,1,1] zone axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyTEMlib.kinematic_scattering as ks         # kinematic scattering Library\n",
    "                             # with Atomic form factors from Kirkland's book\n",
    "import ase\n",
    "def h5_add_crystal_structure(h5_file, input_structure):\n",
    "    \n",
    "    if isinstance(input_structure, ase.Atoms):\n",
    "        crystal_tags = ks.get_dictionary(input_structure)\n",
    "        if crystal_tags['metadata'] == {}:\n",
    "            crystal_tags['metadata'] = {'name': input_structure.get_chemical_formula()}\n",
    "    elif isinstance(input_structure, dict):\n",
    "        crystal_tags = input_structure\n",
    "    else:\n",
    "        raise TypeError('Need a dictionary or an ase.Atoms object with ase installed')\n",
    "\n",
    "    structure_group = sidpy.hdf.prov_utils.create_indexed_group(h5_file, 'Structure_')\n",
    "\n",
    "    for key, item in crystal_tags.items():\n",
    "        if not isinstance(item, dict):\n",
    "            structure_group[key] = item\n",
    "\n",
    "    if 'base' in crystal_tags:\n",
    "        structure_group['relative_positions'] = crystal_tags['base']\n",
    "    if 'title' in crystal_tags:\n",
    "        structure_group['title'] = str(crystal_tags['title'])\n",
    "        structure_group['_' + crystal_tags['title']] = str(crystal_tags['title'])\n",
    "\n",
    "    # ToDo: Save all of info dictionary\n",
    "    if 'metadata' in input_structure:\n",
    "        structure_group.create_group('metadata')\n",
    "        sidpy.hdf.hdf_utils.write_simple_attrs(structure_group['metadata'], input_structure['metadata'])\n",
    "\n",
    "    h5_file.file.flush()\n",
    "    return structure_group\n",
    "                                                                                 \n",
    "crystal_tags = ks.structure_by_name('Graphite')\n",
    "h5_add_crystal_structure(current_channel.file, crystal_tags)\n",
    "                                                                                \n",
    "crystal_tags = ks.structure_by_name('ZnO')\n",
    "\n",
    "ft.h5_add_crystal_structure(current_channel.file, crystal_tags)\n",
    "\n",
    "sidpy.hdf_utils.print_tree(current_channel.file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping Track of Analysis and Results\n",
    "A notebook is notorious for getting confusing, especially if one uses different notebooks for different task, but store them in the same file.\n",
    "\n",
    "If you like a result of your calculation, log it.\n",
    "|\n",
    "The function will write your calculation to the pyNSID style file and attaches a time stamp.\n",
    "\n",
    "The two functions below are part of file_tools of pyTEMlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidpy.hdf_utils.print_tree(current_channel.file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dictionary = {'analysis': 'Nothing', 'name': 'Nothing'}\n",
    "\n",
    "log_group = ft.log_results(current_channel, attributes=info_dictionary)\n",
    "\n",
    "sidpy.hdf_utils.print_tree(current_channel.file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example for a log\n",
    "We log the Fourier Transform of the image we loaded\n",
    "\n",
    "First we perform the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_image = current_dataset.fft().abs()\n",
    "fft_image = np.log(60+fft_image)\n",
    "\n",
    "view = fft_image.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we like this we log it.\n",
    "\n",
    "Please note that just saving the fourier transform would not be good as we also need the scale and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fft_image.title = 'FFT Gamma corrected'\n",
    "fft_image.metadata = {'analysis': 'fft'}\n",
    "log_group = ft.log_results(current_dataset, fft_image)\n",
    "\n",
    "ft.h5_tree(current_channel.file)\n",
    "view = fft_image.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "print(current_channel.file.filename)\n",
    "h5_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open h5_file\n",
    "Open the h5_file that we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dataset= ft.open_file()\n",
    "\n",
    "current_channel = dataset.h5_dataset\n",
    "view = dataset.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Short check if we got the data right\n",
    "we print the tree and we plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# See if a tree has been created within the hdf5 file:\n",
    "ft.h5_tree(dataset.h5_dataset.file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## If we are done, we close the pyNSID style file.\n",
    "\n",
    "This is necessary to make the file ready to be opened by another notebook or program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dataset.h5_dataset.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Navigation\n",
    "- <font size = \"3\">  **Back  [Matplotlib and Numpy for Micrographs](CH1_03-Data_Representation.ipynb)** </font>\n",
    "- <font size = \"3\">  **Next: [Overview](CH1_06-Overview.ipynb)** </font>\n",
    "- <font size = \"3\">  **Chapter 1: [Introduction](CH1_00-Introduction.ipynb)** </font>\n",
    "- <font size = \"3\">  **List of Content: [Front](../_MSE672_Intro_TEM.ipynb)** </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": "5",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
